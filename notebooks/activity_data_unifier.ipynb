{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity Data DF:\n",
    "Create a united table with all of the shipments arrived and supplied items over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from functools import reduce\n",
    "\n",
    "def process_two_columns_per_date_file(input_path):\n",
    "    # Read the Excel file and load \"Sheet1\" into a DataFrame\n",
    "    df = pd.read_csv(input_path, header=None, low_memory=False)\n",
    "    \n",
    "    # Extract unique identifiers from the first column\n",
    "    identifiers = df.iloc[:, 0].dropna().unique()\n",
    "    \n",
    "    # Initialize a dictionary to store data for each unique value in the second row\n",
    "    parameters = df.iloc[1, 1:3].values\n",
    "    data_dict = {name: {\"id\": identifiers} for name in parameters}\n",
    "    \n",
    "    # Iterate over the remaining columns\n",
    "    total_columns = len(df.columns[1:])\n",
    "    current_column = 0\n",
    "    \n",
    "    for column in df.columns[1:]:\n",
    "        # Update progress\n",
    "        current_column += 1\n",
    "        # print(f\"Processing column {current_column}/{total_columns}\")\n",
    "        \n",
    "        # Extract the string from the first row of the current column, parse it to a string of format YYYY-MM-DD\n",
    "        date = dt.strptime(df[column].iloc[0],'%d/%m/%Y')\n",
    "        string_date = dt.strftime(date, \"%Y-%m-%d\")\n",
    "        # Extract the data type from the second row\n",
    "        data_type = (df[column].iloc[1])\n",
    "        \n",
    "        # Skip the column if the data type is empty\n",
    "        if pd.isna(data_type):\n",
    "            continue\n",
    "        \n",
    "        # Add the data from the column to the respective dictionary\n",
    "        records = df[column].iloc[2:]#.values\n",
    "        if data_type not in data_dict:\n",
    "            data_dict[data_type] = {string_date: records}\n",
    "        else:\n",
    "            data_dict[data_type][string_date] = records\n",
    "    \n",
    "    dfs = []\n",
    "    for parameter, data in data_dict.items():\n",
    "        data_frame = pd.DataFrame(data)\n",
    "        date_cols = data_frame.columns[1:].tolist()\n",
    "        action_rows_df = pd.melt(\n",
    "            data_frame,\n",
    "            id_vars=['id'],\n",
    "            value_vars=date_cols,\n",
    "            var_name='date',\n",
    "            value_name=parameter\n",
    "        ).dropna()\n",
    "        dfs.append(action_rows_df)\n",
    "\n",
    "    united_df = reduce(lambda df1,df2: pd.merge(df1,df2,on=['id', 'date'], how='outer'), dfs).fillna(0)\n",
    "    return united_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the files in the directory '../data/raw/activity':\n",
    "import os\n",
    "import re\n",
    "\n",
    "def get_files_with_pattern(path, pattern):\n",
    "    files = os.listdir(path)\n",
    "    return [file for file in files if re.search(pattern, file)]\n",
    "\n",
    "path = '../data/raw/activity/'\n",
    "\n",
    "a_files = get_files_with_pattern(path, 'a_')\n",
    "b_files = get_files_with_pattern(path, 'b_')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty list of DFs, perform the processing on each file and append them to the list:\n",
    "a_dfs = []\n",
    "b_dfs = []\n",
    "for dfs, files in [(a_dfs, a_files), (b_dfs, b_files)]:\n",
    "    for file in files:\n",
    "        print(f\"Processing file {file}\")\n",
    "        df = process_two_columns_per_date_file(f\"../data/raw/activity/{file}\")\n",
    "        dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "united_list = a_dfs + b_dfs\n",
    "united_df = pd.concat(united_list)\n",
    "united_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group by id and date and sum the values:\n",
    "grouped_df = united_df.groupby(['id', 'date']).sum().sort_values(by=['date']).reset_index()\n",
    "grouped_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename all columns to lower case, and replace spaces with underscores:\n",
    "grouped_df.columns = [column.lower().replace(' ', '_') for column in grouped_df.columns]\n",
    "# rename the 'id' column to 'uuid':\n",
    "grouped_df.rename(columns={'id': 'uuid'}, inplace=True)\n",
    "# replace every ',' in the DF with '':\n",
    "grouped_df = grouped_df.replace(',', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the result to a csv file under '../data/processed/activity_data.csv':\n",
    "grouped_df.to_csv('../data/processed/activity_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
